### Persistence and shared representations in neural network architectures using RNNs

Representation learning interacts with task switching. The study by Musslick and Cohen (2017) could show by simulations on feedforward neural network that the more a shared representation of a previously executed task persists in time, the more it interferes with a subsequent task. They state that “[...] persistence in neural networks is typically implemented in the form of recurrent connections between the processing units. Here, we chose, for simplicity, to implement persistence by explicitly integrating processed information over time.” With this project we aim to replicate their findings using recurrent neural networks (RNNs).


Literature:

- Musslick, S., Cohen, J.D.: A mechanistic account of constraints on control- dependent processing: Shared representation, conflict and persistence pp. 849–855 (2019)
  
- Musslick, S., Saxe, A.M., Dey, B., Henselman, G., Cohen, J.D.: Multitasking capability versus learning eﬀiciency in neural network architectures p. 829– 834 (2017)
 

Group Members:

- Jenny Arndt: jearndt@uni-osnabrueck.de
  
- Melisa Altinyelek: maltinyelek@uni-osnabrueck.de
