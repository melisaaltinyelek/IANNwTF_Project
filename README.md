### Persistence and shared representations in neural network architectures using RNNs

Representation learning interacts with task switching. The study by Musslick and Cohen (2019) could show by simulations on feedforward neural networks that the more a shared representation of a previously executed task persists in time, the more it interferes with a subsequent task. They state that “[...] persistence in neural networks is typically implemented in the form of recurrent connections between the processing units. Here, we chose, for simplicity, to implement persistence by explicitly integrating processed information over time.” With this project, we aim to build on their findings by implementng persistence and shared representations on a recurrent neural network (RNN). 


Literature:

- Musslick, S., Cohen, J.D.: A mechanistic account of constraints on control- dependent processing: Shared representation, conflict and persistence pp. 849–855 (2019)
  
- Musslick, S., Saxe, A.M., Dey, B., Henselman, G., Cohen, J.D.: Multitasking capability versus learning eﬀiciency in neural network architectures p. 829– 834 (2017)
 

Group Members:

- Jenny Arndt: jearndt@uni-osnabrueck.de
  
- Melisa Altinyelek: maltinyelek@uni-osnabrueck.de
